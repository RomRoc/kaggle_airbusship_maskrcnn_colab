{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ship_classification_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "fsYyl5Q9beh4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "myLnxdV19JV_",
        "colab_type": "code",
        "outputId": "61bd1e29-6895-4553-e922-0ebd7b00e7df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tTn4ddTe-mdn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ]
    },
    {
      "metadata": {
        "id": "UowPgcgJ0dMg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TRAINING 1: 5000-0.0001-182555\n",
        "#TRAINING 2: 10000-0.00001-182555\n",
        "#TRAINING 3: 15000-0.000001-182555\n",
        "\n",
        "\n",
        "l_rate = 0.000001\n",
        "n_steps=15000\n",
        "\n",
        "\n",
        "# TRAIN\n",
        "#_NUM_VALIDATION = 10000\n",
        "#SPLITS_TO_SIZES = {'train': 182555, 'validation': 10000}\n",
        "\n",
        "\n",
        "# TEST\n",
        "_NUM_VALIDATION = 0\n",
        "SPLITS_TO_SIZES = {'train': 192555, 'validation': 0}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5RAK5aiShCTW",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "f03edeb2-0892-4a79-af1c-409ff5305988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#DOWNLOAD MODEL FROM DRIVE IF NEEDED FOR TRAIN OR DETECT\n",
        "\n",
        "%cd\n",
        "\n",
        "!pip install -q PyDrive\n",
        "\n",
        "modelFileId = '1pxrrcTEijmivHPKTBilWZLkD9pmDKLFz'\n",
        "modelFilePath = 'model.zip'\n",
        "train_dir = '/tmp/inception_finetuned/'\n",
        "\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id': modelFileId})\n",
        "downloaded.GetContentFile(modelFilePath)\n",
        "print('Files downloaded')\n",
        "\n",
        "ds = ZipFile(modelFilePath)\n",
        "ds.extractall(train_dir)\n",
        "print('Files extracted')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "Files downloaded\n",
            "Files extracted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YWjjl5U1LQWD",
        "colab_type": "code",
        "outputId": "38dc3dce-fb60-4798-b69c-3ae0aeecd13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%cd\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gU-nRzwZgaNM",
        "colab_type": "code",
        "outputId": "fc454dc0-1f2c-4d1b-e89e-10dea21f5ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "%cd\n",
        "\n",
        "!pip install kaggle\n",
        "!git clone https://github.com/tensorflow/models/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/9b/ac57e15fbb239c6793c8d0b7dfd1a4c4a025eaa9f791b5388a7afb515aed/kaggle-1.5.0.tar.gz (53kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.10.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Collecting python-slugify (from kaggle)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Collecting Unidecode>=0.04.16 (from python-slugify->kaggle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 5.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
            "  Running setup.py bdist_wheel for kaggle ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8b/21/3b/a0076243c6ae12a6215b2da515fe06b539aee7217b406e510e\n",
            "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
            "Successfully built kaggle python-slugify\n",
            "Installing collected packages: Unidecode, python-slugify, kaggle\n",
            "Successfully installed Unidecode-1.0.22 kaggle-1.5.0 python-slugify-1.2.6\n",
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 1, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 22343 (delta 0), reused 0 (delta 0), pack-reused 22342\u001b[K\n",
            "Receiving objects: 100% (22343/22343), 559.85 MiB | 27.76 MiB/s, done.\n",
            "Resolving deltas: 100% (13202/13202), done.\n",
            "Checking out files: 100% (2829/2829), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nLGCSWkHHQTX",
        "colab_type": "code",
        "outputId": "d6e0b2d5-bef1-430d-cfc2-8d4ffcf38d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "%cd\n",
        "\n",
        "!kaggle competitions download -c airbus-ship-detection\n",
        "\n",
        "!unzip -q -d train train_v2.zip\n",
        "!unzip -q train_ship_segmentations_v2.csv.zip\n",
        "\n",
        "!mkdir test\n",
        "!mkdir test/ship\n",
        "!mkdir test/noship\n",
        "!unzip -q -d test/noship test_v2.zip\n",
        "\n",
        "!rm *.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "Downloading sample_submission_v2.csv to /root\n",
            "  0% 0.00/274k [00:00<?, ?B/s]\n",
            "100% 274k/274k [00:00<00:00, 55.9MB/s]\n",
            "Downloading train_ship_segmentations_v2.csv.zip to /root\n",
            " 83% 15.0M/18.0M [00:00<00:00, 33.9MB/s]\n",
            "100% 18.0M/18.0M [00:00<00:00, 60.1MB/s]\n",
            "Downloading test_v2.zip to /root\n",
            "100% 2.12G/2.12G [00:19<00:00, 148MB/s]\n",
            "\n",
            "Downloading train_v2.zip to /root\n",
            "100% 26.4G/26.4G [03:58<00:00, 145MB/s]\n",
            "100% 26.4G/26.4G [03:58<00:00, 119MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EUzMSCu0MNoo",
        "colab_type": "code",
        "outputId": "6bf10bb1-4198-4a0d-ab81-6afa0cd52c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd ~/models/research/slim\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from datasets import dataset_utils\n",
        "\n",
        "# Seed for repeatability.\n",
        "_RANDOM_SEED = 0\n",
        "\n",
        "# The number of shards per dataset split.\n",
        "_NUM_SHARDS = 5\n",
        "\n",
        "train_dir = '/tmp/inception_finetuned/'\n",
        "checkpoints_dir = '/tmp/checkpoints'\n",
        "testdatadir = '/root/test/'\n",
        "\n",
        "\n",
        "class ImageReader(object):\n",
        "  \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    # Initializes function that decodes RGB JPEG data.\n",
        "    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
        "    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
        "\n",
        "  def read_image_dims(self, sess, image_data):\n",
        "    image = self.decode_jpeg(sess, image_data)\n",
        "    return image.shape[0], image.shape[1]\n",
        "\n",
        "  def decode_jpeg(self, sess, image_data):\n",
        "    image = sess.run(self._decode_jpeg,\n",
        "                     feed_dict={self._decode_jpeg_data: image_data})\n",
        "    assert len(image.shape) == 3\n",
        "    assert image.shape[2] == 3\n",
        "    return image\n",
        "\n",
        "\n",
        "def _get_filenames_and_classes(dataset_dir):\n",
        "  \"\"\"Returns a list of filenames and inferred class names.\n",
        "  Args:\n",
        "    dataset_dir: A directory containing a set of subdirectories representing\n",
        "      class names. Each subdirectory should contain PNG or JPG encoded images.\n",
        "  Returns:\n",
        "    A list of image file paths, relative to `dataset_dir` and the list of\n",
        "    subdirectories, representing class names.\n",
        "  \"\"\"\n",
        "  ship_root = dataset_dir\n",
        "  directories = []\n",
        "  class_names = []\n",
        "  for filename in os.listdir(ship_root):\n",
        "    path = os.path.join(ship_root, filename)\n",
        "    if os.path.isdir(path):\n",
        "      directories.append(path)\n",
        "      class_names.append(filename)\n",
        "\n",
        "  photo_filenames = []\n",
        "  for directory in directories:\n",
        "    for filename in os.listdir(directory):\n",
        "      path = os.path.join(directory, filename)\n",
        "      photo_filenames.append(path)\n",
        "\n",
        "  return photo_filenames, sorted(class_names)\n",
        "\n",
        "\n",
        "def _get_dataset_filename(dataset_dir, split_name, shard_id):\n",
        "  output_filename = 'ships_%s_%05d-of-%05d.tfrecord' % (\n",
        "      split_name, shard_id, _NUM_SHARDS)\n",
        "  return os.path.join(dataset_dir, output_filename)\n",
        "\n",
        "\n",
        "def image_to_tfexample(image_data, image_format, height, width, class_id, filename):\n",
        "  return tf.train.Example(features=tf.train.Features(feature={\n",
        "      'image/encoded': dataset_utils.bytes_feature(image_data),\n",
        "      'image/format': dataset_utils.bytes_feature(image_format),\n",
        "      'image/class/label': dataset_utils.int64_feature(class_id),\n",
        "      'image/height': dataset_utils.int64_feature(height),\n",
        "      'image/width': dataset_utils.int64_feature(width),\n",
        "      'image/filename': dataset_utils.bytes_feature(filename.encode('utf8')),\n",
        "  }))\n",
        "\n",
        "\n",
        "def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n",
        "  \"\"\"Converts the given filenames to a TFRecord dataset.\n",
        "  Args:\n",
        "    split_name: The name of the dataset, either 'train' or 'validation'.\n",
        "    filenames: A list of absolute paths to png or jpg images.\n",
        "    class_names_to_ids: A dictionary from class names (strings) to ids\n",
        "      (integers).\n",
        "    dataset_dir: The directory where the converted datasets are stored.\n",
        "  \"\"\"\n",
        "  assert split_name in ['train', 'validation']\n",
        "\n",
        "  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
        "\n",
        "  with tf.Graph().as_default():\n",
        "    image_reader = ImageReader()\n",
        "\n",
        "    with tf.Session('') as sess:\n",
        "\n",
        "      for shard_id in range(_NUM_SHARDS):\n",
        "        output_filename = _get_dataset_filename(\n",
        "            dataset_dir, split_name, shard_id)\n",
        "\n",
        "        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
        "          start_ndx = shard_id * num_per_shard\n",
        "          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n",
        "          for i in range(start_ndx, end_ndx):\n",
        "            try:\n",
        "              sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\n",
        "                  i+1, len(filenames), shard_id))\n",
        "              sys.stdout.flush()\n",
        "\n",
        "              # Read the filename:\n",
        "              image_data = tf.gfile.FastGFile(filenames[i], 'rb').read()\n",
        "              height, width = image_reader.read_image_dims(sess, image_data)\n",
        "\n",
        "              class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
        "              class_id = class_names_to_ids[class_name]\n",
        "\n",
        "              example = image_to_tfexample(\n",
        "                  image_data, b'jpg', height, width, class_id, filenames[i])\n",
        "              tfrecord_writer.write(example.SerializeToString())\n",
        "            except:\n",
        "              print('\\nError file:',filenames[i])\n",
        "\n",
        "  sys.stdout.write('\\n')\n",
        "  sys.stdout.flush()\n",
        "\n",
        "\n",
        "def _clean_up_temporary_files(dataset_dir):\n",
        "  \"\"\"Removes temporary files used to create the dataset.\n",
        "  Args:\n",
        "    dataset_dir: The directory where the temporary files are stored.\n",
        "  \"\"\"\n",
        "  filename = _DATA_URL.split('/')[-1]\n",
        "  filepath = os.path.join(dataset_dir, filename)\n",
        "  tf.gfile.Remove(filepath)\n",
        "\n",
        "  tmp_dir = dataset_dir\n",
        "  tf.gfile.DeleteRecursively(tmp_dir)\n",
        "\n",
        "\n",
        "def _dataset_exists(dataset_dir):\n",
        "  for split_name in ['train', 'validation']:\n",
        "    for shard_id in range(_NUM_SHARDS):\n",
        "      output_filename = _get_dataset_filename(\n",
        "          dataset_dir, split_name, shard_id)\n",
        "      if not tf.gfile.Exists(output_filename):\n",
        "        return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def run_conversion(dataset_dir):\n",
        "  \"\"\"Runs the download and conversion operation.\n",
        "  Args:\n",
        "    dataset_dir: The dataset directory where the dataset is stored.\n",
        "  \"\"\"\n",
        "  if not tf.gfile.Exists(dataset_dir):\n",
        "    tf.gfile.MakeDirs(dataset_dir)\n",
        "\n",
        "  if _dataset_exists(dataset_dir):\n",
        "    print('Dataset files already exist. Exiting without re-creating them.')\n",
        "    return\n",
        "\n",
        "#  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n",
        "  photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n",
        "  class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n",
        "\n",
        "  # Divide into train and test:\n",
        "  random.seed(_RANDOM_SEED)\n",
        "  random.shuffle(photo_filenames)\n",
        "  training_filenames = photo_filenames[_NUM_VALIDATION:]\n",
        "  validation_filenames = photo_filenames[:_NUM_VALIDATION]\n",
        "  with open('validation_filenames.txt', 'w') as f:\n",
        "    for item in validation_filenames:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "  # First, convert the training and validation sets.\n",
        "  _convert_dataset('train', training_filenames, class_names_to_ids,\n",
        "                   dataset_dir)\n",
        "  _convert_dataset('validation', validation_filenames, class_names_to_ids,\n",
        "                   dataset_dir)\n",
        "\n",
        "  # Finally, write the labels file:\n",
        "  labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n",
        "  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n",
        "\n",
        "  #_clean_up_temporary_files(dataset_dir)\n",
        "  print('\\nFinished converting the Ships dataset!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/models/research/slim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UHlzVzaJ7UdS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from datasets import dataset_utils\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "_FILE_PATTERN = 'ships_%s_*.tfrecord'\n",
        "\n",
        "_NUM_CLASSES = 2\n",
        "\n",
        "_ITEMS_TO_DESCRIPTIONS = {\n",
        "    'image': 'A color image of varying size.',\n",
        "    'label': 'A single integer between 0 and 1',\n",
        "}\n",
        "\n",
        "\n",
        "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n",
        "  \"\"\"Gets a dataset tuple with instructions for reading ships.\n",
        "  Args:\n",
        "    split_name: A train/validation split name.\n",
        "    dataset_dir: The base directory of the dataset sources.\n",
        "    file_pattern: The file pattern to use when matching the dataset sources.\n",
        "      It is assumed that the pattern contains a '%s' string so that the split\n",
        "      name can be inserted.\n",
        "    reader: The TensorFlow reader type.\n",
        "  Returns:\n",
        "    A `Dataset` namedtuple.\n",
        "  Raises:\n",
        "    ValueError: if `split_name` is not a valid train/validation split.\n",
        "  \"\"\"\n",
        "  if split_name not in SPLITS_TO_SIZES:\n",
        "    raise ValueError('split name %s was not recognized.' % split_name)\n",
        "\n",
        "  if not file_pattern:\n",
        "    file_pattern = _FILE_PATTERN\n",
        "  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
        "\n",
        "  # Allowing None in the signature so that dataset_factory can use the default.\n",
        "  if reader is None:\n",
        "    reader = tf.TFRecordReader\n",
        "\n",
        "  keys_to_features = {\n",
        "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
        "      'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n",
        "      'image/class/label': tf.FixedLenFeature(\n",
        "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
        "      'image/filename': tf.FixedLenFeature((), tf.string, default_value='png'),\n",
        "  }\n",
        "\n",
        "  items_to_handlers = {\n",
        "      'image': slim.tfexample_decoder.Image(),\n",
        "      'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
        "      'filename': slim.tfexample_decoder.Tensor('image/filename'),\n",
        "  }\n",
        "\n",
        "  decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
        "      keys_to_features, items_to_handlers)\n",
        "\n",
        "  labels_to_names = None\n",
        "  if dataset_utils.has_labels(dataset_dir):\n",
        "    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n",
        "\n",
        "  return slim.dataset.Dataset(\n",
        "      data_sources=file_pattern,\n",
        "      reader=reader,\n",
        "      decoder=decoder,\n",
        "      num_samples=SPLITS_TO_SIZES[split_name],\n",
        "      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
        "      num_classes=_NUM_CLASSES,\n",
        "      labels_to_names=labels_to_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FTDO0rSiI94p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from preprocessing import inception_preprocessing\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "\n",
        "def load_batch(dataset, batch_size=32, height=299, width=299, is_training=False):\n",
        "    \"\"\"Loads a single batch of data.\n",
        "    \n",
        "    Args:\n",
        "      dataset: The dataset to load.\n",
        "      batch_size: The number of images in the batch.\n",
        "      height: The size of each image after preprocessing.\n",
        "      width: The size of each image after preprocessing.\n",
        "      is_training: Whether or not we're currently training or evaluating.\n",
        "    \n",
        "    Returns:\n",
        "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
        "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
        "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
        "    \"\"\"\n",
        "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
        "        dataset, common_queue_capacity=32,\n",
        "        common_queue_min=8)\n",
        "    image_raw, label, filename = data_provider.get(['image', 'label', 'filename'])\n",
        "    \n",
        "    # Preprocess image for usage by Inception.\n",
        "    if is_training:\n",
        "      image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training)\n",
        "    else:\n",
        "      image = inception_preprocessing.preprocess_for_eval(image_raw, height, width, central_fraction=1)\n",
        "    \n",
        "    # Preprocess the image for display purposes.\n",
        "    image_raw = tf.expand_dims(image_raw, 0)\n",
        "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
        "    image_raw = tf.squeeze(image_raw)\n",
        "\n",
        "    # Batch it up.\n",
        "    images, images_raw, labels, filenames = tf.train.batch(\n",
        "          [image, image_raw, label, filename],\n",
        "          batch_size=batch_size,\n",
        "          num_threads=1,\n",
        "          capacity=2 * batch_size)\n",
        "    \n",
        "    return images, images_raw, labels, filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bAMrRVcfdnZo",
        "colab_type": "code",
        "outputId": "6d3db8b6-0be6-424d-acbd-ec858a8218a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "%cd\n",
        "\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "os.remove('train/6384c3e78.jpg')\n",
        "\n",
        "delete_files =  []\n",
        "for f in delete_files:\n",
        "        os.remove('/root/test/noship/'+f)\n",
        "\n",
        "traindatadir = '/root/train/'\n",
        "df = pd.read_csv('train_ship_segmentations_v2.csv')\n",
        "print('Move',df['ImageId'].nunique(),'file')\n",
        "\n",
        "ship_dir = traindatadir + 'ship'\n",
        "os.mkdir(ship_dir)\n",
        "dfNotNull = df[df.EncodedPixels.notnull()]\n",
        "imageIdsShip = dfNotNull['ImageId'].unique()\n",
        "for file_name in imageIdsShip:\n",
        "  shutil.move(traindatadir + file_name, ship_dir)\n",
        "print('Moved in',ship_dir,imageIdsShip.size,'file')\n",
        "\n",
        "noship_dir = traindatadir + 'noship'\n",
        "os.mkdir(noship_dir)\n",
        "for file_name in glob.glob(traindatadir+\"/*.jpg\"):\n",
        "  shutil.move(file_name, noship_dir)\n",
        "print('Moved in',noship_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "Move 192556 file\n",
            "Moved in /root/train/ship 42556 file\n",
            "Moved in /root/train/noship\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qti8M4YGaczE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train"
      ]
    },
    {
      "metadata": {
        "id": "gLHRyB48d4PL",
        "colab_type": "code",
        "outputId": "812fe727-62a0-451a-ec44-238559b9c653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#DOWNLOAD IF YOU DONT NEED TO START FROM MY PRETRAINED MODEL\n",
        "\n",
        "%cd ~/models/research/slim\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from datasets import dataset_utils\n",
        "\n",
        "# Main slim library\n",
        "from tensorflow.contrib import slim\n",
        "from datasets import dataset_utils\n",
        "\n",
        "url = \"http://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz\"\n",
        "\n",
        "if not tf.gfile.Exists(checkpoints_dir):\n",
        "    tf.gfile.MakeDirs(checkpoints_dir)\n",
        "\n",
        "dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/models/research/slim\n",
            ">> Downloading inception_v4_2016_09_09.tar.gz 100.0%\n",
            "Successfully downloaded inception_v4_2016_09_09.tar.gz 171177982 bytes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vm-pQZyQObwB",
        "colab_type": "code",
        "outputId": "489ec6e5-652a-4937-e3db-c24d64efa7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "%cd ~/models/research/slim\n",
        "\n",
        "traindatadir = '/root/train/'\n",
        "run_conversion(traindatadir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/models/research/slim\n",
            ">> Converting image 1/182555 shard 0WARNING:tensorflow:From <ipython-input-7-bfc03b39ec18>:123: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.gfile.GFile.\n",
            ">> Converting image 182555/182555 shard 4\n",
            ">> Converting image 10000/10000 shard 4\n",
            "\n",
            "Finished converting the Ships dataset!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "csu0JiIIvHqA",
        "colab_type": "code",
        "outputId": "bc960b70-5f30-42f7-bf98-68727464568f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2672
        }
      },
      "cell_type": "code",
      "source": [
        "%cd ~/models/research/slim\n",
        "\n",
        "import os\n",
        "\n",
        "from nets import inception\n",
        "from preprocessing import inception_preprocessing\n",
        "\n",
        "from tensorflow.contrib import slim\n",
        "image_size = inception.inception_v4.default_image_size\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def get_init_fn():\n",
        "    \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
        "    checkpoint_exclude_scopes=[\"InceptionV4/Logits\", \"InceptionV4/AuxLogits\"]\n",
        "    \n",
        "    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
        "\n",
        "    variables_to_restore = []\n",
        "    for var in slim.get_model_variables():\n",
        "        for exclusion in exclusions:\n",
        "            if var.op.name.startswith(exclusion):\n",
        "                break\n",
        "        else:\n",
        "            variables_to_restore.append(var)\n",
        "\n",
        "    return slim.assign_from_checkpoint_fn(\n",
        "      os.path.join(checkpoints_dir, 'inception_v4.ckpt'),\n",
        "      variables_to_restore)\n",
        "\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    \n",
        "    dataset = get_split('train', traindatadir)\n",
        "    images, _, labels, filenames = load_batch(dataset, batch_size = batch_size, height=image_size, width=image_size)\n",
        "\n",
        "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
        "    with slim.arg_scope(inception.inception_v4_arg_scope()):\n",
        "        logits, _ = inception.inception_v4(images, num_classes=dataset.num_classes, is_training=True)\n",
        "        \n",
        "    # Specify the loss function:\n",
        "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
        "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
        "    total_loss = slim.losses.get_total_loss()\n",
        "\n",
        "    # Create some summaries to visualize the training process:\n",
        "    tf.summary.scalar('losses/Total_Loss', total_loss)\n",
        "  \n",
        "    # Specify the optimizer and create the train op:\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n",
        "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
        "    \n",
        "    # Run the training:\n",
        "    final_loss = slim.learning.train(\n",
        "        train_op,\n",
        "        logdir=train_dir,\n",
        "        init_fn=get_init_fn(),\n",
        "        log_every_n_steps=100,\n",
        "        number_of_steps=n_steps)\n",
        "        \n",
        "    print('Finished training. Last batch loss %f' % final_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/models/research/slim\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:242: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:94: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
            "WARNING:tensorflow:From <ipython-input-9-d81ff00438f6>:43: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "WARNING:tensorflow:From <ipython-input-12-ff563567af54>:44: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
            "Instructions for updating:\n",
            "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:399: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
            "Instructions for updating:\n",
            "Use tf.losses.compute_weighted_loss instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:147: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
            "Instructions for updating:\n",
            "Use tf.losses.add_loss instead.\n",
            "WARNING:tensorflow:From <ipython-input-12-ff563567af54>:45: get_total_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
            "Instructions for updating:\n",
            "Use tf.losses.get_total_loss instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:258: get_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
            "Instructions for updating:\n",
            "Use tf.losses.get_losses instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:260: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
            "Instructions for updating:\n",
            "Use tf.losses.get_regularization_losses instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "INFO:tensorflow:Restoring parameters from /tmp/inception_finetuned/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting Session.\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Starting Queues.\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "INFO:tensorflow:Recording summary at step 10000.\n",
            "INFO:tensorflow:global step 10099: loss = 0.3122 (2.106 sec/step)\n",
            "INFO:tensorflow:global step 10199: loss = 0.3020 (2.114 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.462107\n",
            "INFO:tensorflow:Recording summary at step 10275.\n",
            "INFO:tensorflow:global step 10299: loss = 0.2814 (2.106 sec/step)\n",
            "INFO:tensorflow:global step 10399: loss = 0.3898 (2.134 sec/step)\n",
            "INFO:tensorflow:global step 10499: loss = 0.2929 (2.124 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.46965\n",
            "INFO:tensorflow:Recording summary at step 10556.\n",
            "INFO:tensorflow:global step 10599: loss = 0.2945 (2.118 sec/step)\n",
            "INFO:tensorflow:global step 10699: loss = 0.2963 (2.120 sec/step)\n",
            "INFO:tensorflow:global step 10799: loss = 0.3691 (2.104 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.469353\n",
            "INFO:tensorflow:Recording summary at step 10838.\n",
            "INFO:tensorflow:global step 10899: loss = 0.3005 (2.110 sec/step)\n",
            "INFO:tensorflow:global step 10999: loss = 0.5081 (2.097 sec/step)\n",
            "INFO:tensorflow:global step 11099: loss = 0.2886 (2.097 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 11120.\n",
            "INFO:tensorflow:global_step/sec: 0.468937\n",
            "INFO:tensorflow:global step 11199: loss = 0.2889 (2.108 sec/step)\n",
            "INFO:tensorflow:global step 11299: loss = 0.2997 (2.119 sec/step)\n",
            "INFO:tensorflow:global step 11399: loss = 0.4324 (2.125 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.470389\n",
            "INFO:tensorflow:Recording summary at step 11401.\n",
            "INFO:tensorflow:global step 11499: loss = 0.3264 (2.128 sec/step)\n",
            "INFO:tensorflow:global step 11599: loss = 0.2838 (2.109 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.469437\n",
            "INFO:tensorflow:Recording summary at step 11683.\n",
            "INFO:tensorflow:global step 11699: loss = 0.2921 (2.113 sec/step)\n",
            "INFO:tensorflow:global step 11799: loss = 0.2837 (2.123 sec/step)\n",
            "INFO:tensorflow:global step 11899: loss = 0.2816 (2.102 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.468895\n",
            "INFO:tensorflow:Recording summary at step 11965.\n",
            "INFO:tensorflow:global step 11999: loss = 0.2834 (2.108 sec/step)\n",
            "INFO:tensorflow:global step 12099: loss = 0.3003 (2.103 sec/step)\n",
            "INFO:tensorflow:global step 12199: loss = 0.2798 (2.122 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.469754\n",
            "INFO:tensorflow:Recording summary at step 12246.\n",
            "INFO:tensorflow:global step 12299: loss = 0.2862 (2.112 sec/step)\n",
            "INFO:tensorflow:global step 12399: loss = 0.2880 (2.109 sec/step)\n",
            "INFO:tensorflow:global step 12499: loss = 0.2877 (2.102 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.469818\n",
            "INFO:tensorflow:Recording summary at step 12528.\n",
            "INFO:tensorflow:global step 12599: loss = 0.2842 (2.116 sec/step)\n",
            "INFO:tensorflow:global step 12699: loss = 0.3287 (2.102 sec/step)\n",
            "INFO:tensorflow:global step 12799: loss = 0.3233 (2.110 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 12810.\n",
            "INFO:tensorflow:global step 12899: loss = 0.2920 (2.113 sec/step)\n",
            "INFO:tensorflow:global step 12999: loss = 0.2964 (2.110 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 13092.\n",
            "INFO:tensorflow:global step 13099: loss = 0.2852 (2.121 sec/step)\n",
            "INFO:tensorflow:global step 13199: loss = 0.3875 (2.112 sec/step)\n",
            "INFO:tensorflow:global step 13299: loss = 0.2968 (2.103 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 13374.\n",
            "INFO:tensorflow:global step 13399: loss = 0.2789 (2.114 sec/step)\n",
            "INFO:tensorflow:global step 13499: loss = 0.2842 (2.102 sec/step)\n",
            "INFO:tensorflow:global step 13599: loss = 0.2846 (2.116 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 13656.\n",
            "INFO:tensorflow:global step 13699: loss = 0.3449 (2.124 sec/step)\n",
            "INFO:tensorflow:global step 13799: loss = 0.2809 (2.121 sec/step)\n",
            "INFO:tensorflow:global step 13899: loss = 0.2838 (2.091 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 13937.\n",
            "INFO:tensorflow:global step 13999: loss = 0.2790 (2.121 sec/step)\n",
            "INFO:tensorflow:global step 14099: loss = 0.3046 (2.106 sec/step)\n",
            "INFO:tensorflow:global step 14199: loss = 0.2890 (2.101 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 14219.\n",
            "INFO:tensorflow:global step 14299: loss = 0.2818 (2.118 sec/step)\n",
            "INFO:tensorflow:global step 14399: loss = 0.3832 (2.124 sec/step)\n",
            "INFO:tensorflow:global step 14499: loss = 0.2798 (2.127 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 14501.\n",
            "INFO:tensorflow:global step 14599: loss = 0.2786 (2.117 sec/step)\n",
            "INFO:tensorflow:global step 14699: loss = 0.2890 (2.124 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path /tmp/inception_finetuned/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 14782.\n",
            "INFO:tensorflow:global step 14799: loss = 0.2801 (2.103 sec/step)\n",
            "INFO:tensorflow:global step 14899: loss = 0.2806 (2.123 sec/step)\n",
            "INFO:tensorflow:global step 14999: loss = 0.2814 (2.113 sec/step)\n",
            "INFO:tensorflow:Stopping Training.\n",
            "INFO:tensorflow:Finished training! Saving model to disk.\n",
            "Finished training. Last batch loss 0.281442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GoX330muRNK9",
        "colab_type": "code",
        "outputId": "57b9e984-06f1-4ec5-c21b-0612692fb59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /tmp/inception_finetuned/\n",
        "\n",
        "!zip -r model.zip model.ckpt-15000* graph.pbtxt checkpoint\n",
        "\n",
        "#import zipfile\n",
        "from google.colab import auth\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "\n",
        "#filenames = glob.glob('model.ckpt-'+str(n_steps)+'*')\n",
        "#filenames.append('graph.pbtxt')\n",
        "#filenames.append('checkpoint')\n",
        "\n",
        "#with zipfile.ZipFile('model.zip', 'w') as zipMe:        \n",
        "#  for file in filenames:\n",
        "#    zipMe.write(file, compress_type=zipfile.ZIP_DEFLATED)\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "def save_file_to_drive(name, path):\n",
        "    file_metadata = {\n",
        "      'name': name,\n",
        "      'mimeType': 'application/octet-stream'\n",
        "     }\n",
        "\n",
        "    media = MediaFileUpload(path, \n",
        "                    mimetype='application/octet-stream',\n",
        "                    resumable=True)\n",
        "\n",
        "    created = drive_service.files().create(body=file_metadata,\n",
        "                                   media_body=media,\n",
        "                                   fields='id').execute()\n",
        "\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "    return created\n",
        "  \n",
        "save_file_to_drive('model.zip', 'model.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/inception_finetuned\n",
            "  adding: model.ckpt-15000.data-00000-of-00001 (deflated 7%)\n",
            "  adding: model.ckpt-15000.index (deflated 72%)\n",
            "  adding: model.ckpt-15000.meta (deflated 94%)\n",
            "  adding: graph.pbtxt (deflated 97%)\n",
            "  adding: checkpoint (deflated 76%)\n",
            "File ID: 1pxrrcTEijmivHPKTBilWZLkD9pmDKLFz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '1pxrrcTEijmivHPKTBilWZLkD9pmDKLFz'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "fsYyl5Q9beh4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Detect"
      ]
    },
    {
      "metadata": {
        "id": "LrSEUw8SpDyw",
        "colab_type": "code",
        "outputId": "fe42c61e-81e3-41e3-9b79-8d17ca5c468d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "# CROP TEST IMAGE IN 4 PARTS\n",
        "\n",
        "%cd\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "noship_path = os.path.join(testdatadir, 'noship')\n",
        "\n",
        "for f in os.listdir(noship_path):\n",
        "  f_full = os.path.join(noship_path,f)\n",
        "  img = Image.open(f_full)\n",
        "  area1 = (0, 0, 400, 400)\n",
        "  cropped1 = img.crop(area1)\n",
        "  cropped1.save(f_full + '_1', 'JPEG')\n",
        "  area2 = (368, 0, 767, 400)\n",
        "  cropped2 = img.crop(area2)\n",
        "  cropped2.save(f_full + '_2', 'JPEG')\n",
        "  area3 = (0, 368, 400, 767)\n",
        "  cropped3 = img.crop(area3)\n",
        "  cropped3.save(f_full + '_3', 'JPEG')\n",
        "  area4 = (368, 368, 767, 767)\n",
        "  cropped4 = img.crop(area4)\n",
        "  cropped4.save(f_full + '_4', 'JPEG')\n",
        "  \n",
        "run_conversion(testdatadir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            ">> Converting image 1/78030 shard 0WARNING:tensorflow:From <ipython-input-7-bfc03b39ec18>:123: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.gfile.GFile.\n",
            ">> Converting image 78030/78030 shard 4\n",
            "\n",
            "\n",
            "Finished converting the Ships dataset!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e4trar8RvsuE",
        "colab_type": "code",
        "outputId": "3af99c8b-e4af-45fa-e764-494a6a43cfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# CROP VAL IMAGE IN 4 PARTS\n",
        "# RUN BEFORE run_conversion(traindatadir) IF DIDNT EXECUTED\n",
        "\n",
        "%cd\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "valsplitdatadir = '/root/val_split/'\n",
        "ship_path = 'val_split/ship/'\n",
        "noship_path = 'val_split/noship/'\n",
        "os.mkdir(valsplitdatadir)\n",
        "os.mkdir(ship_path)\n",
        "os.mkdir(noship_path)\n",
        "\n",
        "file_list = []\n",
        "with open('/root/models/research/slim/validation_filenames.txt') as f:\n",
        "  file_list = f.read().splitlines()\n",
        "\n",
        "ship_list = [k for k in file_list if '/ship/' in k]\n",
        "noship_list = [k for k in file_list if '/noship/' in k]\n",
        "\n",
        "for f in ship_list:\n",
        "  shutil.copy(f, ship_path+os.path.basename(f))\n",
        "for f in noship_list:\n",
        "  shutil.copy(f, noship_path+os.path.basename(f))\n",
        "  \n",
        "\n",
        "for f in os.listdir(ship_path):\n",
        "  f_full = ship_path + f\n",
        "  img = Image.open(f_full)\n",
        "  area1 = (0, 0, 400, 400)\n",
        "  cropped1 = img.crop(area1)\n",
        "  cropped1.save(f_full + '_1', 'JPEG')\n",
        "  area2 = (368, 0, 767, 400)\n",
        "  cropped2 = img.crop(area2)\n",
        "  cropped2.save(f_full + '_2', 'JPEG')\n",
        "  area3 = (0, 368, 400, 767)\n",
        "  cropped3 = img.crop(area3)\n",
        "  cropped3.save(f_full + '_3', 'JPEG')\n",
        "  area4 = (368, 368, 767, 767)\n",
        "  cropped4 = img.crop(area4)\n",
        "  cropped4.save(f_full + '_4', 'JPEG')\n",
        "\n",
        "\n",
        "for f in os.listdir(noship_path):\n",
        "  f_full = noship_path + f\n",
        "  img = Image.open(f_full)\n",
        "  area1 = (0, 0, 400, 400)\n",
        "  cropped1 = img.crop(area1)\n",
        "  cropped1.save(f_full + '_1', 'JPEG')\n",
        "  area2 = (368, 0, 767, 400)\n",
        "  cropped2 = img.crop(area2)\n",
        "  cropped2.save(f_full + '_2', 'JPEG')\n",
        "  area3 = (0, 368, 400, 767)\n",
        "  cropped3 = img.crop(area3)\n",
        "  cropped3.save(f_full + '_3', 'JPEG')\n",
        "  area4 = (368, 368, 767, 767)\n",
        "  cropped4 = img.crop(area4)\n",
        "  cropped4.save(f_full + '_4', 'JPEG')\n",
        "\n",
        "  \n",
        "run_conversion(valsplitdatadir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            ">> Converting image 40000/40000 shard 4\n",
            ">> Converting image 10000/10000 shard 4\n",
            "\n",
            "Finished converting the Ships dataset!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1J47PF9Ijhgh",
        "colab_type": "code",
        "outputId": "a897f4b4-4153-49dd-87f4-7dc660ce8d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "%cd ~/models/research/slim\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from nets import inception\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "loops = 5000\n",
        "\n",
        "batch_size = 32\n",
        "image_size = inception.inception_v4.default_image_size\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    \n",
        "#    dataset = get_split('validation', traindatadir)\n",
        "    dataset = get_split('train', testdatadir)\n",
        "#    dataset = get_split('train', valsplitdatadir)\n",
        "    print('dataset.num_samples',dataset.num_samples)\n",
        "    images, images_raw, labels, filenames = load_batch(dataset, height=image_size, width=image_size, is_training=False)\n",
        "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
        "    with slim.arg_scope(inception.inception_v4_arg_scope()):\n",
        "        logits, _ = inception.inception_v4(images, num_classes=dataset.num_classes)\n",
        "\n",
        "    probabilities = tf.nn.softmax(logits)\n",
        "    \n",
        "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
        "    init_fn = slim.assign_from_checkpoint_fn(\n",
        "      checkpoint_path,\n",
        "      slim.get_variables_to_restore())\n",
        "    with tf.Session() as sess:\n",
        "        with slim.queues.QueueRunners(sess):\n",
        "            sess.run(tf.initialize_local_variables())\n",
        "            init_fn(sess)\n",
        "            with open('inference_result.csv', 'w') as f:\n",
        "                f.write('filename,prediction\\n')\n",
        "                for i in range(loops):\n",
        "                    sys.stdout.write('\\rLoop: '+str(i))\n",
        "                    sys.stdout.flush()\n",
        "                    np_probabilities, np_images_raw, np_labels, np_filenames = sess.run([probabilities, images_raw, labels, filenames])\n",
        "                    for i in range(batch_size): \n",
        "                        image = np_images_raw[i, :, :, :]\n",
        "                        true_label = np_labels[i]\n",
        "                        predicted_label = np.argmax(np_probabilities[i, :])\n",
        "                        predicted_name = dataset.labels_to_names[predicted_label]\n",
        "                        true_name = dataset.labels_to_names[true_label]\n",
        "                        filename = os.path.basename(np_filenames[i].decode())\n",
        "                        f.write(filename + ',' + predicted_name+'\\n')\n",
        "#                print('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
        "#                plt.figure()\n",
        "#                plt.imshow(image.astype(np.uint8))\n",
        "#                plt.title('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
        "#                plt.axis('off')\n",
        "#                plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/models/research/slim\n",
            "dataset.num_samples 192555\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:242: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:94: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
            "WARNING:tensorflow:From <ipython-input-9-d81ff00438f6>:43: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_local_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.local_variables_initializer` instead.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/inception_finetuned/model.ckpt-15000\n",
            "Loop: 4999"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vobUIQIITK4N",
        "colab_type": "code",
        "outputId": "a03fef69-9348-4377-deec-243a8cd7762e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%cd ~/models/research/slim\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "df = pd.read_csv('inference_result.csv')\n",
        "inference_unique = df.drop_duplicates()\n",
        "print('inference_unique shape:',inference_unique.shape)\n",
        "inference_unique.to_csv('inference_classification.csv', index=False)\n",
        "\n",
        "#files.download('inference_result.csv')\n",
        "files.download('inference_classification.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/models/research/slim\n",
            "inference_unique shape: (79749, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}